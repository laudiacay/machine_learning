I’m not really sure how to organize this writeup, so I’m just going to go through the different files:

First, img.py. This imports the training images in load_data, and the test image in load_trial image. It also computes the integral images according to the formula in the powerpoint in compute_integral_image, and computes the integral images for all the images in the dataset in compute_integral_images. Finally, intensity computes the intensity of a feature based on the formula in kondor’s powerpoint, and mark_image puts the red squares on the test image to show faces.

Then I have feature.py, which is a class that takes features and evaluates them on images. It generates features (the 2x1 and 1x2 ones) in gen_feature_list, and finds the area of half a feature in find_area to use in Img’s intensity. My opt_p_theta in feature.py didn’t work for whatever reason, even though I sent it to a TA and he said it was fine… so I also have another featureWithoutSklearn.py, which is my implementation of features as described in Kondor’s lecture, but feature.py uses sklearn’s DecisionTreeClassifier instead of the p-theta implementation. It’s the same thing under the hood since I have it set to do only one layer of the decision tree. eval_learner and error_rate just do exactly what their names say, calculating the learner’s values and the error rate of a given feature, and error_rate_lambda just rearranges the arguments of error_rate so that it can be used with Multiprocessing.Pool. Finally, opt_weaklearner trains the weak learner and picks the one with the smallest error rate, then returns it with some details about it. I parallelized this function using multiprocessing because it took forever. I trained with features that were a step of 3 apart in width and location

In stronglearner.py, I have two classes, Stump (which is the weak learners that are linearly combined in the StrongLearner), and StrongLearner, which stores the parameters for the strong learners and calculates big theta to minimize false negatives. I set big theta to -0.999 * min(false_negs) because sometimes using -1 would throw out all my correctly classified negatives to fix one weird false negative, and this setting made it converge much better. I also have a few functions to get various error statistics and evaluate the StrongLearner.

Cascade.py evaluates images through the whole cascade of strong learners. It also has functions to calculate the cascade’s success rate, and to get the indices of images that are correctly classified backgrounds so that they can be thrown out of the training set for training the next strong learner.

Finally, viola_jones.py implements adaboost and putting together the cascade. Adaboost_round trains one strong learner on the dataset, using pretty much the same algorithm as from the slides. On lines 27-30 I add or subtract machine epsilon from the error so it doesn’t break math.log if it’s at 0 or 1. In compute_cascade, I either load part of a cascade saved from a previous run to a file, or I initialize it with no strong learners. While the success rate is less than 99.5% and it’s still improving, I keep adding strong learners to the cascade by calling the adaboost_round function, then I remove the correct backgrounds, save my work, and train again. At the bottom I have some stuff to restore partial/full trained cascades from files, then more stuff to detect faces in the test images or restore the faces it found from a file. Finally it saves “test2.png” which is the marked up image showing where faces were detected, which is in the zip i submitted.

My final cascade had 5 strong learners. Each of those had 3 weak learners-- this is because I set a minimum of 3 weak learners per strong learner in order to prevent weird bugs when setting big theta. It took about 4 minutes to train each weak learner, so I think it probably took about an hour to train the whole thing with parallelization across 5 cores (I’m not entirely sure of the exact time, I started it and then left for a few hours and came back later).

